<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="LoRA: How can I tune my model without an expensive GPU?" /><meta name="author" content="Alvaro" /><meta property="og:locale" content="en" /><meta name="description" content="Training neural networks can be really tedious. Both in terms of methodological complexity – tuning hyperparameters, syncing training runs, scheduling learning rates, etc. – and also hardware requirements." /><meta property="og:description" content="Training neural networks can be really tedious. Both in terms of methodological complexity – tuning hyperparameters, syncing training runs, scheduling learning rates, etc. – and also hardware requirements." /><link rel="canonical" href="https://airlegend.github.io/posts/lora/" /><meta property="og:url" content="https://airlegend.github.io/posts/lora/" /><meta property="og:site_name" content="Alvaro’s blog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-10-30T04:33:00+01:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="LoRA: How can I tune my model without an expensive GPU?" /><meta name="twitter:site" content="@aalvaroir" /><meta name="twitter:creator" content="@Alvaro" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Alvaro"},"dateModified":"2023-10-30T04:33:00+01:00","datePublished":"2023-10-30T04:33:00+01:00","description":"Training neural networks can be really tedious. Both in terms of methodological complexity – tuning hyperparameters, syncing training runs, scheduling learning rates, etc. – and also hardware requirements.","headline":"LoRA: How can I tune my model without an expensive GPU?","mainEntityOfPage":{"@type":"WebPage","@id":"https://airlegend.github.io/posts/lora/"},"url":"https://airlegend.github.io/posts/lora/"}</script><title>LoRA: How can I tune my model without an expensive GPU? | Alvaro's blog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Alvaro's blog"><meta name="application-name" content="Alvaro's blog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src=" https://avatars.githubusercontent.com/u/9653892?v=4 " alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Alvaro's blog</a></div><div class="site-subtitle font-italic">Notes on projects and ideas</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/AIRLegend" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/aalvaroir" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['alvaroibrain','outlook.es'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>LoRA: How can I tune my model without an expensive GPU?</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>LoRA: How can I tune my model without an expensive GPU?</h1><div class="post-meta text-muted"><div> By <em> Alvaro </em></div><div class="d-flex"><div> <span> Posted <em class="timeago" data-ts="1698636780" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2023-10-30 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1575 words"> <em>8 min</em> read</span></div></div></div><div class="post-content"><p>Training neural networks can be really tedious. Both in terms of methodological complexity – tuning hyperparameters, syncing training runs, scheduling learning rates, etc. – and also hardware requirements.</p><p>That’s why historically large architectures (like <a href="https://arxiv.org/abs/1512.03385">ResNet</a>) have been “pretrained”, or trained on broad tasks (image classification, segmentation, object detection, etc) by more-or-less powerful organizations – like Google – and then, practitioners froze the original weights adding only a few layers at the last part of the network which were the ones being trained.</p><p>This approach enabled researchers, individuals and even companies to leverage previously learned knowledge in those pretrained models.</p><h2 id="how-do-i-finetune-without-a-high-end-gpu--tpu"><span class="mr-2">How do I finetune without a high end GPU / TPU?</span><a href="#how-do-i-finetune-without-a-high-end-gpu--tpu" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>Nonetheless, the explosion of big models like LLMs or <a href="https://airlegend.github.io/posts/diffusion-models/">diffusion models</a> made even more difficult to the general public to gather the necessary compute power for training them from scratch. Only a handful of powerful companies are capable of doing this.</p><p>So, in order to do it, a similar <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">scheme emerged</a>: pretraining big models on very generalist tasks (like text modeling), usually in an unsupervised (or semisupervised) setting, releasing those weights to the public and let practitioners further train them on more specific tasks (for example, running a few more epochs on their own corpus).</p><p>This, while drastically reducing the required amount of data and compute time, left us with the “compute memory problem”. Most of these new models, even in their simplest versions, barely fit on most consumer-level GPUs (like a RTX3080). (<em>NOTE: This is usually addressed by setting up multi-GPU settings</em>)</p><p>The problem of being unable to fit these models usually arises when running backpropagation, when it’s necessary to store a copy on memory of all the partial derivatives of all the matrix weights, usually doubling the memory footprint.</p><p>Several hacks have been proposed for tacking or easing this constraint, but the one I’m talking about on this post is <a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a>.</p><h2 id="what-is-lora"><span class="mr-2">What is LoRA?</span><a href="#what-is-lora" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>LoRA, or “Low Rank Adaptation” is a very clever yet useful technique which allows us to train big models with less memory than if we fully trained the model as usual. Just for reference, using this method, one can reduce the amount of memory for training Stable Diffusion XL from ~32GB down to less than 8GB (and also proportionally faster!).</p><h2 id="how-does-this-method-work"><span class="mr-2">How does this method work?</span><a href="#how-does-this-method-work" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>When training a neural net, on each backpropagation run we’re basically calculating a delta of the weights on each layer ($\Delta W$) which are added to the previously weights to update them. In an equation form this is expressed as:</p>\[\begin{align*} W_{new} = W_{old} + \Delta W \end{align*}\]<p>The main idea starts here: we can decompose the matrix $\Delta W$ into two (smaller) matrices: $W_A$ and $W_B$. That’s to say, we’re factorizing them! And we can rewrite the it as:</p>\[\begin{align*} W_{new} = W_{old} + \Delta W = W_{old} + W_B W_A \end{align*}\]<p>This matrices should be “rectangular” with one common dimension: $W_A \in \mathbb{R}^{r \times d}; W_B \in \mathbb{R}^{k \times r}$. $d$ and $k$ match the original dimensions of the weight matrix and $r \ll \min(d, k)$.</p><p>So, if you noticed the equations above, the number of entries on the original matrix $\Delta W$ is far greater than the $W_A$ and $W_B$ matrices combined. For example, with a $10 \times 10$ sized $\Delta W$, we can create $W_A, W_B$ of $5 \times 2$ and $2 \times 5$ respectively ($10 \times 10 = 100 \gg 10 + 10 = 20$). It’s not hard to imagine this difference will grow bigger and bigger with larger $\Delta W$ matrices.</p><p>This method is mainly applied on Dense layers (with are the easiest to apply to). During training, we freeze the original layers, and train $W_A$ and $W_B$ via regular backprop and, after being done, we recompose the original $\Delta W$ by multiplying the two decomposed matrices $W_B W_A$, which in turn we sum to the original weights ($W_{old}$)</p><h2 id="okay-nice-but-why-does-it-work"><span class="mr-2">Okay, nice. But WHY does it work?</span><a href="#okay-nice-but-why-does-it-work" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>Previously, papers like <a href="https://arxiv.org/pdf/1803.03635.pdf">The lottery ticket hypothesis</a> have stated that, after trained, neural networks contain sparse weights and can be pruned in order to reduce their size. Related with that, research <a href="https://arxiv.org/pdf/2012.13255.pdf">Aghajanyan et al.</a> pointed out that, after fine-tuning, LLM’s layers could be further reduced in size and still maintain a good performance.</p><p>Translated to maths, this mean that layers get “an intrinsic low dimension”, or that weight matrices have not full rank (i.e. the same “knowledge” could be reproduced with fewer dimensions because some of them are being squeezed after the transformation). If we can remove the column dependencies of the matrix, we save compute and training time!</p><p>The last question is: <em>how do we chose “$r$”? Is there a rule? Does it have a tradeoff choosing a smaller “$r$”?</em></p><p>Well, researchers study this effect in the paper by applying LoRA with different “$r$” to the self-attention layers of GPT-2/3 and evaluating it in several datasets and found no noticeable differences (see tables 5 and 6 of the paper), which could mean that, even compressing the matrices, the same knowledge for the downstream tasks is preserved.</p><h2 id="implementation"><span class="mr-2">Implementation</span><a href="#implementation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>Let’s see a very straightforward example on how to LoRA is applied to the training of a very simple model consisting on only one transformation matrix. The same idea can be applied to larger models by selecting Dense layers and substituting them for LoRA layers, though.</p><div class="language-python highlighter-rouge"><div class="code-header"><span> </span><button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>


<span class="c1"># Let us suppose a pretrained network with shape input_dim x output_dim
</span><span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> 


<span class="c1"># Just for reference, we build a "target" W matrix which represents the 
# final (finetuned) version of the W matrix. As stated by the literature, this
# matrix isn't full-rank, so we artificially set it to have 99 linear combinations
# of the first column.
</span><span class="n">W_finetuned</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">W_finetuned</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*=</span> <span class="n">i</span>
</pre></table></code></div></div><p><em>Note the $W$ matrix has frozen weights, thus, we won’t be retraining it and the gradients for it won’t be computed and held into memory.</em></p><p>Then, we initialize the 2 weight sub-matrices as shown in the LoRA paper</p><div class="language-python highlighter-rouge"><div class="code-header"><span> </span><button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="n">rank</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># The rank 'r' for this low-rank adaptation
</span>
<span class="n">W_B</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="n">W</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">rank</span><span class="p">))</span> <span class="c1"># LoRA weight B
</span><span class="n">W_A</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">W</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span> <span class="c1"># LoRA weight A
</span>
<span class="c1"># Initialization of LoRA weights
</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">W_B</span><span class="p">);</span>
<span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">W_A</span><span class="p">);</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Number of original parameters: </span><span class="si">{</span><span class="n">W</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">W</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Number of LoRA parameters: </span><span class="si">{</span><span class="n">W_B</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">W_B</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">W_A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">W_A</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="code-header"><span> </span><button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>Number of original parameters: 10000
Number of LoRA parameters: 800
</pre></table></code></div></div><p>Let’s create a dummy dataset for training (finetuning) the sub-matrices</p><div class="language-python highlighter-rouge"><div class="code-header"><span> </span><button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="c1"># We create a dummy training dataset (consinsting only on one batch)
</span><span class="n">input_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">32</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>      

<span class="c1"># We simulate a target dataset (in wich we finetune) by passing the batch through
# the "non-full-rank" matrix.
</span><span class="n">target_batch</span> <span class="o">=</span> <span class="n">input_batch</span> <span class="o">@</span> <span class="n">W_finetuned</span>

<span class="k">print</span><span class="p">((</span><span class="n">input_batch</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">target_batch</span><span class="p">.</span><span class="n">shape</span><span class="p">))</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="code-header"><span> </span><button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>(torch.Size([32, 100]), torch.Size([32, 100]))
</pre></table></code></div></div><p>Now, we train the $W_A$ and $W_B$ matrices as told on the paper.</p><div class="language-python highlighter-rouge"><div class="code-header"><span> </span><button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">W_A</span><span class="p">,</span> <span class="n">W_B</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1_000</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">input_batch</span> <span class="o">@</span> <span class="p">(</span><span class="n">W</span> <span class="o">+</span> <span class="p">(</span><span class="n">W_B</span> <span class="o">@</span> <span class="n">W_A</span><span class="p">))</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">l1_loss</span><span class="p">(</span><span class="n">target_batch</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="nb">reduce</span><span class="o">=</span><span class="s">'mean'</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">losses</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">loss</span><span class="p">.</span><span class="n">detach</span><span class="p">().</span><span class="n">numpy</span><span class="p">()))</span>

<span class="k">print</span><span class="p">(</span><span class="n">losses</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:])</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="code-header"><span> </span><button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>[0.5827709436416626, 0.6526086330413818, 0.6803646683692932]
</pre></table></code></div></div><p>Now, we update the original $W$ matrix by summing the product of the factorized matrices:</p><div class="language-python highlighter-rouge"><div class="code-header"><span> </span><button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="n">alpha</span> <span class="o">=</span> <span class="n">rank</span>
<span class="n">scaling_factor</span> <span class="o">=</span> <span class="n">alpha</span><span class="o">/</span><span class="n">rank</span>  <span class="c1"># in this example the scaling factor is 1
</span>
<span class="c1"># Update parameters!
</span><span class="n">W</span> <span class="o">+=</span> <span class="p">(</span><span class="n">W_B</span> <span class="o">@</span> <span class="n">W_A</span><span class="p">)</span> <span class="o">*</span> <span class="n">scaling_factor</span>
</pre></table></code></div></div><p>If everything has gone well, the updated $W$ matrix should be very similar as the expected one (<code class="language-plaintext highlighter-rouge">W_finetuned</code>). Let’s check it out:</p><div class="language-python highlighter-rouge"><div class="code-header"><span> </span><button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="k">print</span><span class="p">(</span><span class="s">"Finetuned should be:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">W_finetuned</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Actual W is:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="code-header"><span> </span><button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre><td class="rouge-code"><pre>Finetuned should be:
tensor([[ 1.,  1.,  2.,  ..., 97., 98., 99.],
        [ 1.,  1.,  2.,  ..., 97., 98., 99.],
        [ 1.,  1.,  2.,  ..., 97., 98., 99.],
        ...,
        [ 1.,  1.,  2.,  ..., 97., 98., 99.],
        [ 1.,  1.,  2.,  ..., 97., 98., 99.],
        [ 1.,  1.,  2.,  ..., 97., 98., 99.]])

Actual W is:
tensor([[ 1.9471,  1.0072,  1.9705,  ..., 96.9968, 97.9976, 98.9610],
        [ 1.0144,  1.9725,  2.0391,  ..., 97.0116, 98.0161, 99.0161],
        [ 0.9610,  1.0239,  2.9211,  ..., 96.9896, 97.9768, 98.9119],
        ...,
        [ 0.9898,  0.9909,  2.0054,  ..., 97.9946, 98.0018, 98.9830],
        [ 0.9855,  0.9858,  2.0140,  ..., 96.9801, 98.9850, 98.9716],
        [ 0.9781,  1.0012,  1.9835,  ..., 96.9722, 97.9784, 99.9474]],
       grad_fn=&lt;AddBackward0&gt;)
</pre></table></code></div></div><p>Nice! As you can see, the matrices are pretty close despite the fact we’ve finetuned with only 8% of the original number of parameters!</p><p>If you want to learn more about how LoRA can be applied to your models, check out this repo: https://github.com/microsoft/LoRA. It contains re-implementations of common layer types supporting LoRA out of the box.</p><h2 id="closing-up"><span class="mr-2">Closing up</span><a href="#closing-up" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>LoRA is a very interesting idea that closes us, the mere mortals, the ability of playing with big models (and also makes it cheap!) by applying a simple matrix decomposition approach on Dense layers, which are currently present on most models using Attention mechanisms. There are also further improvements over LoRA, for example <a href="https://arxiv.org/abs/2305.14314">QLoRA</a>, which is a Quantized version of LoRA that reduces even more the memory footprint!</p><p>Hopefully, we’ll see how these kind of techniques enable new practitioners enter the community!</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='//categories/ml/'>ML</a>, <a href='//categories/deep-learning/'>Deep Learning</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="//tags/maths/" class="post-tag no-text-decoration" >maths</a> <a href="//tags/programming/" class="post-tag no-text-decoration" >programming</a> <a href="//tags/python/" class="post-tag no-text-decoration" >python</a> <a href="//tags/pytorch/" class="post-tag no-text-decoration" >pytorch</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=LoRA%3A+How+can+I+tune+my+model+without+an+expensive+GPU%3F+-+Alvaro%27s+blog&url=https%3A%2F%2Fairlegend.github.io%2Fposts%2Flora%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=LoRA%3A+How+can+I+tune+my+model+without+an+expensive+GPU%3F+-+Alvaro%27s+blog&u=https%3A%2F%2Fairlegend.github.io%2Fposts%2Flora%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fairlegend.github.io%2Fposts%2Flora%2F&text=LoRA%3A+How+can+I+tune+my+model+without+an+expensive+GPU%3F+-+Alvaro%27s+blog" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/dreambooth/">How I generated professional headshots for my LinkedIn profile</a><li><a href="/posts/openchords/">The AI Coder: Building a website using AI</a><li><a href="/posts/lcm/">Latent Consistency Models: How to speedup image generation</a><li><a href="/posts/ai-development/">Some thoughts on the usage of AI for developing software: Is the end near?</a><li><a href="/posts/forward-mode-autodiferentiation/">The magic behind (forward mode) Automatic Differentiation</a></ul></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/dreambooth/"><div class="card-body"> <em class="timeago small" data-ts="1700274780" > 2023-11-18 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>How I generated professional headshots for my LinkedIn profile</h3><div class="text-muted small"><p> The other day I was bored, wandering through LinkedIn and realized that I probably needed some new profile pics. You know, the ones very corporate-guys share on their posts. Something cool with a s...</p></div></div></a></div><div class="card"> <a href="/posts/lcm/"><div class="card-body"> <em class="timeago small" data-ts="1701142380" > 2023-11-28 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Latent Consistency Models: How to speedup image generation</h3><div class="text-muted small"><p> In a previous post I briefly covered Diffusion Models. And as I pointed out, they have an important drawback that makes them slow at creating new images. We often have to wait minutes for an image ...</p></div></div></a></div><div class="card"> <a href="/posts/forward-mode-autodiferentiation/"><div class="card-body"> <em class="timeago small" data-ts="1647574380" > 2022-03-18 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>The magic behind (forward mode) Automatic Differentiation</h3><div class="text-muted small"><p> Demystifying Automatic Differentiation If you recall from highschool you’ll probably remember the definition of a derivative: [\begin{equation} f’(x) = \lim_{\epsilon \rightarrow 0} \frac{f(...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="//posts/diffusion-models/" class="btn btn-outline-primary" prompt="Older"><p>Intro to Diffusion Models</p></a> <a href="//posts/dreambooth/" class="btn btn-outline-primary" prompt="Newer"><p>How I generated professional headshots for my LinkedIn profile</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/aalvaroir">Alvaro</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/programming/">programming</a> <a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/maths/">maths</a> <a class="post-tag" href="/tags/pytorch/">pytorch</a> <a class="post-tag" href="/tags/ai/">ai</a> <a class="post-tag" href="/tags/coding/">coding</a> <a class="post-tag" href="/tags/gpt/">gpt</a> <a class="post-tag" href="/tags/cv/">cv</a> <a class="post-tag" href="/tags/image/">image</a> <a class="post-tag" href="/tags/vision/">vision</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/en.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'], ], tags: 'ams' } }; // MathJax = { // tex: { // inlineMath: [ /* start/end delimiter pairs for in-line math */ // ['$','$'], // ['\\(','\\)'] // ], // displayMath: [ /* start/end delimiter pairs for display math */ // ['$$', '$$'], // ['\\[', '\\]'] // ] // }, // chtml: { // scale: 1.2, // global scaling factor for all expressions // }, // svg: { // minScale: .7, // }, // options: { // enableMenu: true, // set to false to disable the menu // menuOptions: { // settings: { // zoom: 'DoubleClick', // zscale: '300%', // zoom scaling factor // texHints: true, // collapsible: false, // true if complex math should be collapsible // explorer: false, // true if the expression explorere should be active // } // } // } // }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
