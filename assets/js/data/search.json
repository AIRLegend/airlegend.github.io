[ { "title": "Latent Consistency Models: How to speedup image generation", "url": "/posts/lcm/", "categories": "ML, Deep Learning", "tags": "maths, programming, python, pytorch", "date": "2023-11-28 04:33:00 +0100", "snippet": "In a previous post I briefly covered Diffusion Models. And as I pointed out, they have an important drawback that makes them slow at creating new images. We often have to wait minutes for an image to be generated (even with beefy machines!). I‚Äôm talking about the ‚Äúdenoising‚Äù steps they have to run for generating an image.‚ÄìDenoising steps of a Diffusion Model to produce an image‚ÄìReducing the generation latency is one of the key problems that have to be solved for this kind of model to become ubiquitous.Recently, a very interesting tool came to the mainstream scene: krea.ai. It has a very interesting set of features that I consider steps forward towards the ideal way we‚Äôll interact with diffusion models. Namely, one of the most interesting is its inference speed in the generation process, making it almost real time. See a quick demo of what I‚Äôm talking: How is this even possible?. Well. One of the key techniques that makes these generations run as fast are the so called ‚ÄúLatent Consistency Models‚Äù, which come from this paper: Song et al. 2023. That‚Äôs what I‚Äôm covering in this post.What is a Consistency Model? (Diclaimer: I will take some mathematical licenses so this post is more legible for a profane reader. If you feel you need more rigorous or detailed explanations, I do refer you to the original paper.)A consistency model is simply a model that tries to predict the initial state of a ‚ÄúProbability Flow‚Äù. If you recall the Diffusion Models post, I talked about an iterative ‚Äúnoising/denoising‚Äù process applied to an initial image. This ‚Äúnoising/denoising‚Äù is made by sampling random noise at each step and adding it to the image. This is the ‚ÄúProbability Flow‚Äù ‚Äì the conjunction of all noise probability distributions at each step.So, as I said above, a Consistency Model tries to approximate ODEs (Ordinary Differential Equations) that takes us from any point in de ‚Äúnoising chain‚Äù back to the beginning of it. I think this image is worth a thousand words:So, as you can see in the top part of the image we have an ODE that describes the change the image follows over time until it reaches the ‚Äúpure noise state‚Äù (the ‚ÄúProbability Flow ODE arrow‚Äù). On the other hand, at the bottom, we have red arrows representing the ‚Äúdeinoising‚Äù ODEs from each point on the chain to the beginning (the ‚Äúnoiseless image‚Äù). As you see, the function that model this ODEs have a $\\theta$ subscript, pointing us that they‚Äôll be characterized by some kind of model.In other words, this model will take an image at a point $T$ and will yield the original image, no matter how big is $T$.Do you see the improvement? With this we could generate images in one run ‚Äì much faster!Important note: Consistency models act on the Pixel space (i.e, the original images). However, most of the Diffusion Models which are SOTA, work on the embedding (latent space). So, the Latent Consistency Models [ Luo et al. 2023] operate on the embedding space in which diffusion is applied (in models like Stable Diffusion) rather than on the image space (which is the classical space), but the idea is almost the same.(Fun fact: They‚Äôre called Consistency Models as their output should be consistent with different $T$s; i.e. on the same trajectory*).How do they work?Supposing we have a well trained consistency model $f_{\\theta}(x, T)$, we‚Äôll be able to pass it a random noise image ($\\hat{x}_T$) and some timestep $T$ in the chain (which we don‚Äôt necessarily have to know, as with the regular Diffusion Process) and it will instantly yield us the $\\hat{x}_0$ image (i.e. the final generation) using a single forward run. Mathematically:\\[\\begin{align*} \\hat{x}_T \\sim \\mathcal{N}(0, I); \\\\ \\hat{x}_0 = f_{\\theta}(\\hat{x}_T, T)\\end{align*}\\]It‚Äôs worth pointing out, however, that this $f_\\theta$ model can have errors (and sometimes it cannot have enough information for estimating the initial state). So the authors suggest that it can also be used multiple times. That is: denoising for a few steps (for example: 10) using the Consistency Model, run another few steps using the regular diffusion denoising procedure, run the Consistency model again‚Ä¶ and so on. Or also applying this consistency model several times on different $T$‚ÄôsHow are they trained?There are two ways of training Consistency Models: 1) Consistency Distillation and 2) Model in Isolation1. Training Consistency Models via DistillationOversimplifying, the way this works is: Pick an image $x_0$ from the dataset. Generate its ‚Äútrajectory‚Äù towards the pure noise ($x_{0..T}$) Use a numerical ODE solver (such as Euler or Heun) to calculate the ODE between pairs of points $(\\hat{x_t}, x_{t+1})$. (Here $\\hat{x}$ denotes the denoised image produced by a pretrained diffusion model) Fit the $f_{\\theta}$ consistency model with those pair of points and the Consistency Distillation Loss (or $\\mathcal{L}^{N}_{CD}$)\\[\\begin{align*} \\mathcal{L}^{N}_{CD}(\\theta, \\theta^{-}, \\phi) = \\mathbb{E}[\\lambda(t_n) d(f_\\theta(x_{t+1}, t_{n+1}), f_{\\theta^-}(x_{t}, t_{n}) ) ]\\end{align*}\\]The above function seems quite verbose, but let‚Äôs take a look at each one of the terms: $\\theta^{-}$: is just a running average of the previous weights of the model and it‚Äôs not optimized. $\\phi$: The ‚Äúdenoiser‚Äù model (aka. the diffusion model) $\\lambda$: Is just a weighting function. Actually, researchers fix it to 1 always. $d$: Is a metric function defined in the range [0, $\\infty$) and measures how similar the two samples are (0 = equal, infinite=totally different). Just for reference, functions that satisfy this requirement are the L2 and L1 distances.2. Training Consistency Models via Model in IsolationThis ‚Äútraining mode‚Äù is presented with the idea of not having to rely in a pretrained diffusion model which estimates $\\nabla\\log p_t(x_t)$.With some mathematical magic (proofs are in the appendix), they can replace the prediction of the diffusion model with the following expression:\\[\\begin{align*} \\nabla\\log p_t(x_t) \\approx \\frac{-x_t - x}{t^2}\\end{align*}\\]where $p_t$ is the probabilty flow for going backwards in the chain.ResultsHere I share some of the results of the technique, but from the Latent Consistency Models paper, which is what‚Äôs actually used on all the big generative models:Just after being trained, with a few steps, we can generate images of impressive quality!Playing with itRecently, a HuggingFace space has been created for demonstrating how image generation is instantly done with this technique. You can play with it here! https://huggingface.co/spaces/ilumine-AI/LCM-PainterI also recorded one of the interactions I made with it. All of it is real time! ü§ØClosing upIn this post I briefly covered Consistency Models, which is a very clever idea that dramatically improves the quality of the interactions we have with image generation models. This is the kind of contributions that will redefine the future on how we‚Äôll work with them!(29/11/2023: Edit post writing)Stability.ai just published SDXL-turbo which is a version of Stable Diffusion that runs in nearly real time with very good results (even improving LCM‚Äôs).The main advancement behind this achievement has been the introduction of a new method called Adversarial Diffusion Distillation. Worth checking it out!" }, { "title": "Some thoughts on the usage of AI for developing software: Is the end near?", "url": "/posts/ai-development/", "categories": "Programming, AI, Development", "tags": "programming, python, gpt, coding, ai", "date": "2023-11-27 08:33:00 +0100", "snippet": "‚Äì¬†Beware of the following to be quite philosophycal, if you're pragmatic, end the reading here! ‚ÄìIn the last post I wrote about how I used AI as an assistant to build a guitar chord website. During that (short) venture I had the opportunity to reflect first hand on the future impact of AI on software development. So that‚Äôs what I‚Äôm sharing on this post.One of my first thoughts, and also one of the most popular, is that this technology, yet incipient, is having a great and rapid impact on lots of industries: finance, journalism, copywriting, ‚Ä¶, and also software development.I‚Äôve heard different opinions on the matter. Some defend that AI will simply be some sort of ‚ÄúGitHub Copilot‚Äù for the software engineers, whereas others state that it‚Äôll completely replace them in the near future and its advancement should be constrained, so the potential impact is slowed.I find it difficult, however, to predict the future ‚Äì¬†how many of us could have guessed GPT4 just 5 or 6 years ago? What makes us think that there won‚Äôt be another breakthrough in the next 5?. Nevertheless, if we assume no future breakthroughs in the near future ‚Äì which is the safest, but still nonsense ‚Äì and the current tech more or less still similar (like the smartphones from year to year), I will point out several arguments on why I think this is not a big deal for the broad tech/programming landscape.First, currently, knowing a framework (or language, or whatever) does not make you more valuable, really. Thinking the opposite is an illusion. And this is more evident when this technology comes into play. Let‚Äôs view it this way. I‚Äôm no frontend developer. I don‚Äôt consider I have advanced notions of developing websites. Therefore, lots of employers wouldn‚Äôt hire me for building websites. Yet, by having access to this tool I‚Äôve been able to build a (simple) frontend in just 10 minutes. Doesn‚Äôt that mean that using it I could perform as a frontend developer professionally?This is great for me, as now I have access to a broader offer of jobs that previously I would struggle with. For example: I could be a Haskell developer in just the time it takes me to write a few queries to ChatGPT.And it‚Äôs also great for companies, as their ‚Äúoffer‚Äù for workers has been increased, greatly reducing the time and effort required to find candidates.Besides that, having the market enlarged both ways (workers and employers) forces the average salary offer to increase.As for the individual contributors, it‚Äôll obviously help. They‚Äôll reduce the time dealing with errors, searching for documentation, reducing the number of bugs and also strengthening and unifying the code style (if you review Pull Requests on a daily basis you‚Äôll probably appreciate the code you check coming from the same AI assistant).Another common argument among the ‚Äúdoomers‚Äù (or neo-ludites) is that, if all of the above is true and productivity is abruptly increased, companies will choose to fire developers as they can get the same job done with fewer people.I don‚Äôt deny that it can be the case in some small companies with delicate financial situations. However, from a broad perspective, this has never been the case. If we take other previous technological breakthroughs (like machinery, robotics, internet or computers) as examples, we find that, even though they were invented, right now, we‚Äôre at the best level of employment humanity ever had [1] [2]. And not to mention that we work less and produce more [3].More specifically, if we focus on robotics/industrial machinery in the US, we find that almost no relative employment has been destroyed in the ‚ÄúManufacturing‚Äù category [4], contrary to what the yielders of the above argument could expect.Returning to the software world, there have previously been tools like IDEs, (rough) automations, assistants, frameworks, languages, etc. that greatly eased the development of programs, and we didn‚Äôt ever see a decrease in programming positions despite all these advancements, quite the opposite.We can even take a look at my field, which is Data Science, or ML. Did AutoML destroy data scientist jobs? Nope. Did the lots of ‚Äústandard pipeline automation solutions‚Äù? Neither. Even after time has passed, the number of positions for Data Scientists is one with the biggest expected growth [5].History teaches us that once we got a new technology that increased productivity, we exploited it to increase our overall production (or efficiency), never to stay the same. That‚Äôs why I think this kind of tech will be used by the industry to accelerate digitalizacion, build new products, companies and services ‚Äì apart from also improving the quality of life of engineers (whose often feel stressed [6] or lose lots of time searching for answers[7])It‚Äôs also worth pointing out that the above doesn‚Äôt take into account ‚Äújob transformations‚Äù or new positions that will appear (e.g. the relentlessly mentioned ‚Äúprompt engineer‚Äù) or how the same positions will change. This is another advantage of the impact of the technology.In summary, the integration of AI in the programming industry is rapidly reshaping the landscape of software development. Despite varied opinions on its potential to replace or augment developers, the current trajectory suggests a democratization of skills and expanded opportunities. The shift towards valuing problem-solving abilities over specific frameworks challenges traditional notions of professional worth. This technological advancement, historically consistent with others like robotics and the internet, is more likely to spur overall employment growth than job reduction. Historically, automation tools have not led to a decline in opportunities, but rather sustained growth. Beyond job displacement concerns, the impact of AI on the industry signifies a catalyst for innovation, digitalization acceleration, and improved quality of life for engineers. The evolution of roles and the emergence of new positions further underscore the positive potential of technology-induced changes in the job market.We should not fear innovation, but intelligently embrace it." }, { "title": "The AI Coder: Building a website using AI", "url": "/posts/openchords/", "categories": "Programming, AI, Development", "tags": "programming, python, gpt, coding, ai", "date": "2023-11-25 03:33:00 +0100", "snippet": "Last week I decided to tackle a quite silly side-project I left stopped about 2 years ago: building my own (guitar) chord / tab site so I can store all my music sheets.The main motivation for doing this is that I got tired of using UltimateGuitar, from which I‚Äôve been being a user since around 2013. Back then it was vey handy to have a site where I could store all my guitar tabs, search for other people‚Äôs and have a rating on whether each one of them was good or not. They also had a nice phone app, Tabs Pro, which I loved, but got removed a few years ago (or deprecated, really) in favor of their current one, ‚ÄúTabs‚Äù.My main issue with it is that their current business goals have become abusive and made them to add lots of things that, IMO, completely ruin the experience. I could write a lot more about this, but here are some points: Their latest price for the Pro version is $10 / month, which, as an amateur guitar player, is not worthy for me. Their mobile app is very bad. It randomly crashes, is very bloated with things I‚Äôll never use (like its TikTok clone for sharing clips of yourself playing). You must pass a nonsense review process for every tab/sheet you upload, even if it‚Äôs only for yourself, which forces you to rewrite (tag for them, really) it in their own format They claim they pay fees and licenses for all their tabs on their site, but it‚Äôs easy to find content scrapped from other smaller sites without even adding a reference.The thing is that, at first it was bearable. For example, they had a nice program that removed the ads for a month if you shared a tab (but it was discontinued).Even with all the inconveniences, and no similar alternatives, I paid the ‚ÄúPro‚Äù tier for a few months in the hope those issues got fixed. But they didn‚Äôt.Starting the adventure: Getting the dataDuring all my time using UltimateGuitar I stored around 500 tabs there, which are efectively blocked unless I pay the Premium subscription (meaning that I can only access them using their web/app). Some time ago, though, while it was allowed and when I was a subscriptor, I downloaded a fraction of them and stored in a backup. However, there are plenty I dont‚Äôt have.So I had no other option than to use shady techniques to retrieve them back, which I‚Äôm not covering here.The important thing is that I could retrieve most of them (yay!). I‚Äôll try to get the remaining ones in the future, though.Each tab has been downloaded alongside its metadata (votes, stars, chord variations, etc.), which I doubt I‚Äôll ever use. I just want the plain ASCII content of the tabs.Alghough JSON format isn‚Äôt too bad, it‚Äôs probably better to store them in some sort of database so the site/app is easier to maintain. SQLite is quite nice for what I want: privately storing and reading my tabs, not building a wide scale product (if I wanted to do that in the future, migrating from SQLite to something like PosgreSQL would be easy).So, I asked ChatGPT for, given a set of JSON files, inserting them into a SQLite table (I gave it the structure of the table) and it gave me this:#... reading the JSONs...conn = sqlite3.connect(DATABASE_PATH)cursor = conn.cursor()insert_query = \"INSERT INTO sheets (id, songname, artistname, artistid, tuning, capo, rating, chords, votes, content) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"values_to_insert = [ (t['id'], t['songname'], t['artistname'], t['artistid'], t['tuning'], t['capo'], t['rating'], t['chords'], t['votes']) for t in filtered_tabs]cursor.executemany(insert_query, values_to_insert)conn.commit()cursor.close()conn.close()(It still amazes me how it ‚Äúunderstands‚Äù that, for inserting in bulk it has to use the excutemany command instead of a regular execute inside a for loop)After executing the above, I ended up with an .sqlite file will all my songs ü•≥Builidng the backendThe backend is very straightforward. Just a Flask app with a few endpoints to retrieve the data that‚Äôs inside the above described sqlite (just returning the songs in a JSON format, nothing fancy), and two endpoints which render the list of all the available sheets and each one of the tabs via Flask (Jinja) templates.These ‚Äútemplates‚Äù, in case you don‚Äôt know, are just regular HTML files in which you can inject Python variables, for example, the lyrics of a song, its chords, etc. The HTMLs are serverside built and then a rendered version is returned to the client (browser).I could write more about them, but that‚Äôs not my goal with this post, so I‚Äôll refer you to this site, in which they‚Äôre futher explained: https://flask.palletsprojects.com/en/2.3.x/tutorial/templates/.The main issue here is that my frontend development skills are mediocre, so I sought some help here.AI to the rescue: Creating the frontendWe‚Äôre entering in the part that sparked the idea of doing this. The other day I was messing around on Twitter and found some guy sharing this tool called screenshot-to-code. Here is a quick video of what it can do:screenshot-to-code: upload a screenshot of any website, watch as AI progressively builds the html, iteratively improving the generated code by comparing it against the screenshot repeatedly. pic.twitter.com/TCrbJj3VS0&mdash; Siqi Chen (@blader) November 21, 2023 It basically uses an screenshot of a webpage and the GPT4-Vision API to automatically build the HTML, CSS and JS code for the site. I got quite mindblown on how simple and how well it worked on the examples. So I had to try it for this chords site.I began by cloning the repo and setting it up according to the instructions. It basically needs an OpenAI API token and (if you want it) Docker:git clone git@github.com:abi/screenshot-to-code.gitcd screenshot-to-codeecho \"OPENAI_API_KEY=sk-your-key\" &gt; .envdocker-compose up -d --build And I was good to go!So, I thought: what if I build a quick mockup of the website on something like Figma, take an screenshot and pass it to screenshot-to-code? I did it.As you‚Äôll probably see, I won‚Äôt be remembered as a exceptional designer. But I don‚Äôt mind, I only need something simple (the more minimalist, the better).The next step was to pass the screenshot to the aforementioned tool. Here is a quick video of the result I got: It even generated a responsive (mobile) version of the web! ü§ØI then copied the code and adapted it using the aforementioned Flask templates (which consisted basically in putting a %block% in which I would pass the content of the tab):Just in about 10 minutes I covered the all frontend part and almost without coding!Meet OpenChordsAfter connecting all the components I got to the first usable version. I had to add a little bit more of Javascript to the generated UI and also include the ‚Äútemplates‚Äù part for Flask to properly render dynamic versions depending on the selected tab/sheet, but I mostly used ChatGPT for doing it, just to force myself to the ‚ÄúAI assisted‚Äù development workflow.The experience was quite nice. Not everything worked right out of the box, but I only had to make minor changes (sometimes GhatGPT even corrected itself!).This ‚Äúv1‚Äù version is very simple, but functionally, it‚Äôs all what I wanted (and mostly all of what I used on UltimateGuitar). Take a look!I also checked on my phone whether the sheets were displayed properly, and they did! I hadn‚Äôt to make any special adjustement! GPT generated responsive code quite nicely!As for future work, some things I‚Äôll probably take a look at are: Adding a way to post new sheets right from the site. Currently I‚Äôm adding them via a Python script, which isn‚Äôt convenient Creating an iOS app that takes the .sqlite db and gives the same funcionality. All locally. Use GPT4-Vision to parse even older sheets I still have in PDF format.As for now, the site serves me well as a repository, I can say it has been a personal success." }, { "title": "How I generated professional headshots for my LinkedIn profile", "url": "/posts/dreambooth/", "categories": "ML, Deep Learning", "tags": "maths, programming, python, pytorch", "date": "2023-11-18 03:33:00 +0100", "snippet": "The other day I was bored, wandering through LinkedIn and realized that I probably needed some new profile pics. You know, the ones very corporate-guys share on their posts. Something cool with a suit and some office-styled background. So I thought: What if I finetune an image generation model on myself and use it to generate them?How can I generate photos of myself?Just to begin, there are countless of image generation models, being the most famous Midjourney, DALL-e and Stable Diffusion ‚Äì being the latter the only one which is public ‚Äì and we can prompt them to generate custom images! (see some examples on the Lexica browser)Last July, Stability.ai (the company behind the Stable Diffusion models) released the best iteration of their model: Stable Diffusion XL 1.0 (SDXL from now on), which is a pretty big step forward in quality from its previous ‚Äúgold-standard model‚Äù, Stable Diffusion 1.5.So, it would be nice if I could finetune it on several of my photos and use it for generating me in whichever type of pic I wanted.But there are a few problems: I don‚Äôt currently have access to my GPU machine (or any cloud GPU) I don‚Äôt want to spend much on this endeavor I don‚Äôt have too much time to train a ‚Äúcomplex‚Äù model on my old Macbook‚Äôs CPU.The ideal solution then is to finetune this model on Google Colab ‚Äì which is the hardest part‚Äì, download the weights and run inference anywhere with the prompt I want.Finetuning SDXL 1.0The first thing we need to do with any ML model is to gather some data. Regardless, with a model such big and complex as SDXL one could expect hundreds or thousands of images to be required, and it wouldn‚Äôt be wrong. Yet, there are techniques for easing this requirement, being one of them ‚ÄúDreambooth‚Äù, which lets anyone finetune their diffusion model using only approximately 5 images (obviously, the more images and the more diverse they are, the better results one will get).Gathering the dataI quickly ran through my phone‚Äôs gallery just to collect about 12 photos of myself. Trying to get diverse poses, angles, lightning and backgrounds. I‚Äôd probably could have tried using more pics, but I got more-or-less good results with this amount.The next step is to resize them to a common aspect ratio, usually squared. For this I used a generic free photo resizer tool I found on Google, called BIRME (feel free to use if you want, it‚Äôs pretty nice). I personally found better generation results using a size of 1024x1024 pixels, although 512x512 wasn‚Äôt too far behind for me and also requires fewer memory (and maybe also faster).Once done, download the .zip with all the resized photos.Finetuning SDXL on Google ColabI‚Äôve made a Google Colab notebook with all the training code I wrote. I used Huggingface‚Äôs Advanced-Autotrain package, which comes with a very handy script for finetuning SDXL (or any model really) with only one command: autotrain dreambooth. On top of that, it handles a critical thing for being able to do this on the low memory‚Äôs T4 GPU that Colab offers: using LoRA, which I cover on a previous post.The notebook is meant to be self explanatory, but basically, you put your images on the img/ folder, run the training and download the safetensors file, which contains the LoRA weights of your tuned model. After that, you can run inference with them wherever you want (although the notebook contains a section for doing it).SDXL comes with a ‚ÄúRefiner‚Äù model which is meant to ‚Äúfix‚Äù inconsistencies the base model made (for example on eyes, hands, text, etc.). However, while on Google Colab, this model cannot be held into memory simultaneously with the base one. We must save the image, restart the session and run the refiner separately, which it‚Äôs a bummer.If you have Colab Pro, or access to more memory, I encourage you to run both steps. However, in my experience, only with the first one, good results can be attained.I should point out, though, that Colab isn‚Äôt reliable for long train runs and could kill your session even though you don‚Äôt reach the maximum allowed GPU memory consumption. Use the --checkpointing_steps parameters accordingly to regularly save checkpoints of your progress.ResultsAfter having the model trained you can use the ‚Äúinference section‚Äù of the same notebook to produce new images given a prompt. I‚Äôve left some placeholder prompt for generating them, but feel free to explore your own! The prompt engineering part is very important (and pretty much alchemy!), having even more impact on the quality than the finetuning step. I‚Äôm pretty much a newbie on it, but I learnt quite a few tricks from this nice guide: ‚ÄúStable Diffusion prompt: a definitive guide‚Äù. I strongly encourage you check it out!Here are some examples of the generations I got with 1k steps of finetuning and the inference parameters on the notebook (the inference time for each image you generate is about 1 minute).BONUS: Improving face qualityAfter generating the photos, sometimes things as the the eyes are not perfect. They can contain artifacts that make obvious the photos are artificially generated. Even running the refiner model those could stay there, which is kind of a bummer, because the rest of the image can look pretty legit! See an example of what I mean:After investigating this issue I came with a technique (that luckily comes with code) called CodeFormer, which was primarily meant to restore images of faces, but apparently is also commonly used among ‚Äúgeneration artists‚Äù to fix the faces of their generations.You can install it via the instructions on their README, it‚Äôs quite straightforward. After doing it, you can fix the faces of your generated images using the following command (replace the --input-path with one to your own image!)python inference_codeformer.py -w 0.5 --input_path /Users/air/Downloads/generated_image-3.pngAs you probably noticed in the above command, there is the -w parameter thing set to 0.5. This an specific parameter for the method that controls the ‚Äústrength‚Äù of the effect. With heavier values (near 1) the model will ‚Äúoverwrite‚Äù more of the original image and therefore we‚Äôll lose some of details of SDXL (your skin will be softer, you‚Äôll have less wrinkles, freckles, scars, etc) and the image will lose quality. On the other hand, with lower values (near 0) the artifacts couldn‚Äôt be removed. In my tests, most of the time, values around 0.5 were pretty much okay, though! But depending on each particular generation you‚Äôd need to increase it!After running it, the fixed image will be saved by default on the same directory as your CodeFormer installation (inside the results folder).The following image shows the fixed version of the above. Pretty cool, uh?Closing upI know there are more user friendly ways of doing the same thing I shared on this post. For example: Kohya, which is a tool that adds an abstraction layer from all the training code. You can take a look at this Kohya tutorial in case you‚Äôre interested. However, as a ‚Äúcode guy‚Äù myself, I find my way more straightforward to follow as long as one is familiar with Jupyter notebooks and some Python!It‚Äôs very impressive how easy one can use state of the art techniques for something as foolish as generating photos of himself with this few amount of effort ‚Äì free Google Colab, a pre-built training script, open sourced models and a few training photos ‚Äì and get results of unthinkable quality just one or two years ago!There‚Äôs only one question left: what we‚Äôll have two years from now?" }, { "title": "LoRA: How can I tune my model without an expensive GPU?", "url": "/posts/lora/", "categories": "ML, Deep Learning", "tags": "maths, programming, python, pytorch", "date": "2023-10-30 04:33:00 +0100", "snippet": "Training neural networks can be really tedious. Both in terms of methodological complexity ‚Äì tuning hyperparameters, syncing training runs, scheduling learning rates, etc. ‚Äì and also hardware requirements.That‚Äôs why historically large architectures (like ResNet) have been ‚Äúpretrained‚Äù, or trained on broad tasks (image classification, segmentation, object detection, etc) by more-or-less powerful organizations ‚Äì like Google ‚Äì and then, practitioners froze the original weights adding only a few layers at the last part of the network which were the ones being trained.This approach enabled researchers, individuals and even companies to leverage previously learned knowledge in those pretrained models.How do I finetune without a high end GPU / TPU?Nonetheless, the explosion of big models like LLMs or diffusion models made even more difficult to the general public to gather the necessary compute power for training them from scratch. Only a handful of powerful companies are capable of doing this.So, in order to do it, a similar scheme emerged: pretraining big models on very generalist tasks (like text modeling), usually in an unsupervised (or semisupervised) setting, releasing those weights to the public and let practitioners further train them on more specific tasks (for example, running a few more epochs on their own corpus).This, while drastically reducing the required amount of data and compute time, left us with the ‚Äúcompute memory problem‚Äù. Most of these new models, even in their simplest versions, barely fit on most consumer-level GPUs (like a RTX3080). (NOTE: This is usually addressed by setting up multi-GPU settings)The problem of being unable to fit these models usually arises when running backpropagation, when it‚Äôs necessary to store a copy on memory of all the partial derivatives of all the matrix weights, usually doubling the memory footprint.Several hacks have been proposed for tacking or easing this constraint, but the one I‚Äôm talking about on this post is LoRA: Low-Rank Adaptation of Large Language Models.What is LoRA?LoRA, or ‚ÄúLow Rank Adaptation‚Äù is a very clever yet useful technique which allows us to train big models with less memory than if we fully trained the model as usual. Just for reference, using this method, one can reduce the amount of memory for training Stable Diffusion XL from ~32GB down to less than 8GB (and also proportionally faster!).How does this method work?When training a neural net, on each backpropagation run we‚Äôre basically calculating a delta of the weights on each layer ($\\Delta W$) which are added to the previously weights to update them. In an equation form this is expressed as:\\[\\begin{align*} W_{new} = W_{old} + \\Delta W\\end{align*}\\]The main idea starts here: we can decompose the matrix $\\Delta W$ into two (smaller) matrices: $W_A$ and $W_B$. That‚Äôs to say, we‚Äôre factorizing them! And we can rewrite the it as:\\[\\begin{align*} W_{new} = W_{old} + \\Delta W = W_{old} + W_B W_A\\end{align*}\\]This matrices should be ‚Äúrectangular‚Äù with one common dimension: $W_A \\in \\mathbb{R}^{r \\times d}; W_B \\in \\mathbb{R}^{k \\times r}$. $d$ and $k$ match the original dimensions of the weight matrix and $r \\ll \\min(d, k)$.So, if you noticed the equations above, the number of entries on the original matrix $\\Delta W$ is far greater than the $W_A$ and $W_B$ matrices combined. For example, with a $10 \\times 10$ sized $\\Delta W$, we can create $W_A, W_B$ of $5 \\times 2$ and $2 \\times 5$ respectively ($10 \\times 10 = 100 \\gg 10 + 10 = 20$). It‚Äôs not hard to imagine this difference will grow bigger and bigger with larger $\\Delta W$ matrices.This method is mainly applied on Dense layers (with are the easiest to apply to). During training, we freeze the original layers, and train $W_A$ and $W_B$ via regular backprop and, after being done, we recompose the original $\\Delta W$ by multiplying the two decomposed matrices $W_B W_A$, which in turn we sum to the original weights ($W_{old}$)Okay, nice. But WHY does it work?Previously, papers like The lottery ticket hypothesis have stated that, after trained, neural networks contain sparse weights and can be pruned in order to reduce their size. Related with that, research Aghajanyan et al. pointed out that, after fine-tuning, LLM‚Äôs layers could be further reduced in size and still maintain a good performance.Translated to maths, this mean that layers get ‚Äúan intrinsic low dimension‚Äù, or that weight matrices have not full rank (i.e. the same ‚Äúknowledge‚Äù could be reproduced with fewer dimensions because some of them are being squeezed after the transformation). If we can remove the column dependencies of the matrix, we save compute and training time!The last question is: how do we chose ‚Äú$r$‚Äù? Is there a rule? Does it have a tradeoff choosing a smaller ‚Äú$r$‚Äù?Well, researchers study this effect in the paper by applying LoRA with different ‚Äú$r$‚Äù to the self-attention layers of GPT-2/3 and evaluating it in several datasets and found no noticeable differences (see tables 5 and 6 of the paper), which could mean that, even compressing the matrices, the same knowledge for the downstream tasks is preserved.ImplementationLet‚Äôs see a very straightforward example on how to LoRA is applied to the training of a very simple model consisting on only one transformation matrix. The same idea can be applied to larger models by selecting Dense layers and substituting them for LoRA layers, though.import torchimport torch.nn as nnimport torch.nn.functional as F# Let us suppose a pretrained network with shape input_dim x output_dimW = torch.eye(100, requires_grad=False) # Just for reference, we build a \"target\" W matrix which represents the # final (finetuned) version of the W matrix. As stated by the literature, this# matrix isn't full-rank, so we artificially set it to have 99 linear combinations# of the first column.W_finetuned = torch.ones([100, 100])for i in range(1, 100): W_finetuned[:, i] *= iNote the $W$ matrix has frozen weights, thus, we won‚Äôt be retraining it and the gradients for it won‚Äôt be computed and held into memory.Then, we initialize the 2 weight sub-matrices as shown in the LoRA paperrank = 4 # The rank 'r' for this low-rank adaptationW_B = nn.Parameter(torch.empty(W.shape[0], rank)) # LoRA weight BW_A = nn.Parameter(torch.empty(rank, W.shape[1])) # LoRA weight A# Initialization of LoRA weightsnn.init.zeros_(W_B);nn.init.normal_(W_A);print(f\"Number of original parameters: {W.shape[0] * W.shape[1]}\")print(f\"Number of LoRA parameters: {W_B.shape[0] * W_B.shape[1] + W_A.shape[0] * W_A.shape[1]}\")Number of original parameters: 10000Number of LoRA parameters: 800Let‚Äôs create a dummy dataset for training (finetuning) the sub-matrices#¬†We create a dummy training dataset (consinsting only on one batch)input_batch = torch.rand((32, 100)) # We simulate a target dataset (in wich we finetune) by passing the batch through# the \"non-full-rank\" matrix.target_batch = input_batch @ W_finetunedprint((input_batch.shape, target_batch.shape))(torch.Size([32, 100]), torch.Size([32, 100]))Now, we train the $W_A$ and $W_B$ matrices as told on the paper.losses = []optimizer = torch.optim.Adam([W_A, W_B], lr=1e-2)for epoch in range(1_000): optimizer.zero_grad() out = input_batch @ (W + (W_B @ W_A)) loss = F.l1_loss(target_batch, out, reduce='mean') loss.backward() optimizer.step() losses.append(float(loss.detach().numpy()))print(losses[-3:])[0.5827709436416626, 0.6526086330413818, 0.6803646683692932]Now, we update the original $W$ matrix by summing the product of the factorized matrices:alpha = rankscaling_factor = alpha/rank # in this example the scaling factor is 1# Update parameters!W += (W_B @ W_A) * scaling_factorIf everything has gone well, the updated $W$ matrix should be very similar as the expected one (W_finetuned). Let‚Äôs check it out:print(\"Finetuned should be:\")print(W_finetuned)print(\"Actual W is:\")print(W)Finetuned should be:tensor([[ 1., 1., 2., ..., 97., 98., 99.], [ 1., 1., 2., ..., 97., 98., 99.], [ 1., 1., 2., ..., 97., 98., 99.], ..., [ 1., 1., 2., ..., 97., 98., 99.], [ 1., 1., 2., ..., 97., 98., 99.], [ 1., 1., 2., ..., 97., 98., 99.]])Actual W is:tensor([[ 1.9471, 1.0072, 1.9705, ..., 96.9968, 97.9976, 98.9610], [ 1.0144, 1.9725, 2.0391, ..., 97.0116, 98.0161, 99.0161], [ 0.9610, 1.0239, 2.9211, ..., 96.9896, 97.9768, 98.9119], ..., [ 0.9898, 0.9909, 2.0054, ..., 97.9946, 98.0018, 98.9830], [ 0.9855, 0.9858, 2.0140, ..., 96.9801, 98.9850, 98.9716], [ 0.9781, 1.0012, 1.9835, ..., 96.9722, 97.9784, 99.9474]], grad_fn=&lt;AddBackward0&gt;)Nice! As you can see, the matrices are pretty close despite the fact we‚Äôve finetuned with only 8% of the original number of parameters!If you want to learn more about how LoRA can be applied to your models, check out this repo: https://github.com/microsoft/LoRA. It contains re-implementations of common layer types supporting LoRA out of the box.Closing upLoRA is a very interesting idea that closes us, the mere mortals, the ability of playing with big models (and also makes it cheap!) by applying a simple matrix decomposition approach on Dense layers, which are currently present on most models using Attention mechanisms. There are also further improvements over LoRA, for example QLoRA, which is a Quantized version of LoRA that reduces even more the memory footprint!Hopefully, we‚Äôll see how these kind of techniques enable new practitioners enter the community!" }, { "title": "Intro to Diffusion Models", "url": "/posts/diffusion-models/", "categories": "ML, Deep Learning", "tags": "maths, programming, python, image, cv, vision", "date": "2022-07-01 05:33:00 +0200", "snippet": "On Diffusion ModelsIn this post I‚Äôm going to cover an introduction to Diffusion Models (as I don‚Äôt consider myself an expert on them) by summarizing Ho et al (2020). ‚ÄúDenoising Diffusion Probabilistic Models‚Äù. Nonetheless, I hope these notes can ease the learning or refreshment process to some of you. So‚Ä¶ let‚Äôs get to it!Diffusion‚Ä¶ what?If you haven‚Äôt been in a cave this last year and a half you‚Äôve probably seen a great amount of ‚ÄúAI generated‚Äù photos on sites like Twitter or Reddit.Some (DALL-E 2) generated images. Taken from @Dalle2Pics.If that‚Äôs the case, you‚Äôre probably familiar with names like ‚ÄúGLIDE‚Äù, ‚ÄúDALL-E 2‚Äù (by OpenAI) or, more recently, ‚ÄúImagen‚Äù (by Google). Well‚Ä¶ these algorithms share a common heart: Diffusion models.What is a diffusion model?You‚Äôve probably already heard about GANs or VAEs. I‚Äôm not covering them here, but these are examples of famous generative models ‚Äìmodels that generates things, like images in this case‚Äì which have been shown to obtain very realistic results.A diffusion model is (yet) another type of generative model. Its main difference to the former examples is that they‚Äôre fall under the category of ‚Äúautoregressive‚Äù.The main idea of this type of models is having a system that takes random noise and iteratively removes a little bit amount of it at a time until a clear image is left.Diffusion overview (source).The above image shows the complete idea. Basically, we begin with a process that takes a real image, gradually adds noise and then we train a reverse process which learns how to backwards walk the sequence, iteratively removing the noise.After having the reverse process trained, we could feed in random noise and let it generate noiseless images.With this intuition in the head we can dive a little bit more into the technical details.1. Adding noise to an imageDiffusion models (both the forward and reverse steps) are parametrized as Markov Chains in which each step in the sequence, $x_t$ (image) depends only on the directly previous image $x_{t-1}$ (or the next one, $x_{t+1}$, in case we‚Äôre denoising). Let‚Äôs focus first on the noising process.Diffusion overview as a Markov Chain. Image taken from original paper (with a few additions by me).In the above image you can see some $q(‚Ä¶)$‚Äôs. This $q$ is a probability distribution which models the the noising process. On each step $t$ we condition it on the last image we have in our sequence, $x_{t-1}$ and sample from it. This new sample will be the our last image $x_{t-1}$ plus some random noise, which is typically modelled as gaussian.If we assume the added noise is gaussian, we can model the conditioned distributions as:\\[\\begin{equation}q(x_t | x_{t-1}) := \\mathcal{N}(x_t; \\sqrt{1-\\beta_t}x_{t-1}, \\beta_t I)\\label{eq:q_open}\\end{equation}\\]As you can see, each new image will be sampled from a gaussian with mean $\\sqrt{1-\\beta_t}x_{t-1}$ and variance $\\beta_t$.But, what is this $\\beta_t$?. Well, it is a coefficient that increases with time and is always in $[0, 1]$. This is done so when $t \\rightarrow \\infty$ the distribution becomes $\\mathcal{N}(0, I)$, a true noise distribution without any remains of the original image.However, the problem with the formulation of $\\eqref{eq:q_open}$ is that it‚Äôs open, meaning that we have, for each step $x_t$, to compute all the previous steps in the chain ($q(x_1|x0), q(x_2|x1), ‚Ä¶, q(x_{t}|x_{t-1})$), making things quite slow.Luckily, with a bit of math magic (I‚Äôll refer you to the original paper for details), we can reformulate it by introducing two new coefficients: $\\alpha_t = 1 - \\beta_t $ and ${\\bar\\alpha_t = \\prod_{i=1}^{t} \\alpha_i}$. So the new expression is:\\[\\begin{equation}q(x_t | x_0) := \\mathcal{N}(x_t; \\sqrt{\\bar\\alpha_t}x_0, (1-\\bar\\alpha_t) I)\\label{eq:q_closed}\\end{equation}\\]The cool thing about this new closed form formula is that it allows us to obtain any arbitrary step in the chain directly from the initial image $x_0$ (yay! we save steps! üéâ).2. Removing noise from an imageAs we saw above, denoising is also thought as a markov process.Denoising overview as a Markov Chain. Image taken from original paper (with a few additions by me).This time, each image $x_t$ comes from sampling a distribution $p_\\theta$ conditioned on the previous (noisier) image.Probably you‚Äôve noted the $\\theta$ subscript. That‚Äôs hinting this process is related to some sort of model in which $\\theta$ refers to its parameters; opposed to $q$, which is nonparametric.Formally, this reverse process is represented as:\\[\\begin{equation}p_{\\theta}(x_{t-1} | x_t) := \\mathcal{N}(x_{t-1}; \\mu_{\\theta}(x_t, t), \\Sigma_\\theta(x_t, t) I)\\label{eq:p1}\\end{equation}\\]Meaning that each ‚Äúclearer‚Äù image comes from sampling a gaussian with mean $\\mu_{\\theta}(x_t, t)$ and variance $\\Sigma_\\theta(x_t, t)$. Again, the $\\theta$‚Äôs point that those come from two trained models.However, original authors state that, after testing, they found that substituting the $\\Sigma_{\\theta}$ model with the original $\\beta_t$ coefficients from the ‚Äúnoising part‚Äù yielded better images and improved training stability. So $\\eqref{eq:p1}$ becomes a simpler expression:\\[\\begin{equation}p_{\\theta}(x_{t-1} | x_t) := \\mathcal{N}(x_{t-1}; \\mu_{\\theta}(x_t, t), \\beta_t I)\\label{eq:p2}\\end{equation}\\]So, this way we‚Äôd only had to train a model that predicts the means of these gaussians at each step $t$!I‚Äôm not entering into the details, but if you follow equations (8), (9) and (10) of the original paper, you‚Äôll find that $\\mu_\\theta$ of $\\eqref{eq:p2}$ is predicting the following:\\[\\begin{equation}\\mu_{\\theta}(x_t, t) := \\frac{1}{\\alpha_t} (x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}} \\epsilon) \\hspace{1em} ;\\epsilon ~ \\mathcal{N}(0, I)\\label{eq:mu0}\\end{equation}\\]Using this new reparametrization, the only new ‚Äúfree‚Äù parameter is $\\epsilon$. We can leverage this fact for training a model that predicts $\\epsilon$ given an image $x_t$. That is simply to predict the noise that was added to $x_{t-1}$ for getting to $x_t$!\\[\\begin{equation}\\mu_{\\theta}(x_t, t) := \\frac{1}{\\alpha_t} (x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}} \\epsilon_\\theta(x_t, t))\\label{eq:mufinal}\\end{equation}\\]Recall inference is done by sampling from $p_\\theta$, which we can simply do it with:\\[\\begin{equation}x_{t-1} \\sim p_\\theta(x_{t-1} | x_t) = \\frac{1}{\\alpha_t} (x_t - \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}} \\epsilon_\\theta(x_t, t)) + \\sqrt{\\beta_t} z \\hspace{1em} ;z \\sim \\mathcal{N}(0, I)\\label{eq:p_inference}\\end{equation}\\]3. Training a model for denoisingHaving reframed the inference, as shown in $\\eqref{eq:p_inference}$, we only need to have a single model for predicting noise ($\\epsilon_\\theta$). This way, we could train the model simply by using any type of regression loss, like MSE (mean squared error) on the added noise.The training algorithm would work as follows: Choose a random $t \\in (1, 2, ‚Ä¶, T)$. Sample noise $\\epsilon \\sim \\mathcal N(0, I)$. Generate $x_{t+1}$ sampling from $q(x_{t+1} | x_0)$ (as shown in $\\eqref{eq:q_closed}$ using $\\epsilon$); that is: $x_{t+1} = \\sqrt{\\bar\\alpha_t}x_0 + (1-\\bar\\alpha_t) \\epsilon$. Calculate loss: $\\mathcal L = MSE(\\epsilon - \\epsilon_\\theta(x_{t+1}, t))$. Take the gradients w.r.t. $\\mathcal L$ and run regular gradient descent.After training, hopefully, $\\epsilon_\\theta$ will be able to run each denoising step as explained on the previous section.Implementing a (simple) diffusion modelLet‚Äôs see how we can implement a very simple diffusion model using Tensorflow.Note: Original authors had access to a TPU-v3 pod for several hours of training time on the CIFAR-10 and CELEB-A datasets. Since I only have access to my laptop and Google Colab, I‚Äôll limit my example to training over one single image.import numpy as npimport matplotlib.pyplot as pltimport numpy as npimport tensorflow as tffrom tensorflow.keras import layersfrom PIL import ImageIn the original paper researchers use a U-Net architecture based on PixelCNN++ (with more complex stuff like self attention blocks). However, for this example I‚Äôll use a ‚Äúvanilla U-Net‚Äù (shown on the next figure), which makes things simpler, faster to train and will still work reasonably well.UNet architecture diagram.def double_conv_block(x, n_filters): # Conv2D then ReLU activation x = layers.Conv2D(n_filters, 3, padding = \"same\", activation = \"swish\", kernel_initializer = \"he_normal\")(x) # relu act # Conv2D then ReLU activation x = layers.Conv2D(n_filters, 3, padding = \"same\", activation = \"swish\", kernel_initializer = \"he_normal\")(x) # relu act return xdef downsample_block(x, n_filters): f = double_conv_block(x, n_filters) p = layers.MaxPool2D(2)(f) p = layers.Dropout(0.1)(p) return f, pdef upsample_block(x, conv_features, n_filters): # upsample x = layers.Conv2DTranspose(n_filters,3, 2, activation = \"relu\", padding=\"same\")(x) # concatenate x = layers.concatenate([x, conv_features]) # dropout x = layers.Dropout(0.1)(x) # Conv2D twice with ReLU activation x = double_conv_block(x, n_filters) return xdef build_unet_model(img_shape=None): \"\"\" Builds a U-Net accepting 64x64 grayscale images by default \"\"\" img_shape = (64,64,1) if not img_shape else img_shape # inputs inputs = layers.Input(shape=img_shape) # encoder: contracting path - downsample # 1 - downsample f1, p1 = downsample_block(inputs, 64) # 2 - downsample f2, p2 = downsample_block(p1, 128) # 3 - downsample f3, p3 = downsample_block(p2, 256) # 4 - downsample f4, p4 = downsample_block(p3, 512) # 5 - bottleneck bottleneck = double_conv_block(p4, 1024) # decoder: expanding path - upsample # 6 - upsample u6 = upsample_block(bottleneck, f4, 512) # 7 - upsample u7 = upsample_block(u6, f3, 256) # 8 - upsample u8 = upsample_block(u7, f2, 128) # 9 - upsample u9 = upsample_block(u8, f1, 64) # outputs outputs = layers.Conv2D(1, 1, padding=\"same\", activation = \"linear\")(u9) # unet model with Keras Functional API unet_model = tf.keras.Model(inputs, outputs, name=\"U-Net\") return unet_modelOur single image will be the famous ‚ÄúLena‚Äù photo in one single channel (i.e. black and white). Let‚Äôs download it from Wikipedia :Lena photo.!wget https://upload.wikimedia.org/wikipedia/en/7/7d/Lenna_%28test_image%29.png -O lena.png# Load the imagelena_img = Image.open('lena.png').resize((64, 64)).convert('L')lena_imgThe method assumes all input image values lie in $[-1, 1]$# Normalize the image so the values of its pixels lie between [-1, 1]lena = (lena.astype(float) / 255.) * 2 - 1Now we can create arrays with our fixed $\\beta_t$ coefficients (and its $\\alpha$ and $\\bar\\alpha_t$ derivations).betas = np.linspace(1e-4, 0.04, num=300) # linear increase schedulealphas = 1 - betasalphas_bars = np.cumprod(alphas)# Also keep tensor copiesbetas_tf = tf.convert_to_tensor(betas)alphas_tf = tf.convert_to_tensor(alphas)alphas_bars_tf = tf.convert_to_tensor(alphas_bars)Let‚Äôs create the diffusion process, $q$.def q(x0, t, return_noise=False): \"\"\" Gets a noised version of x0 sampling from q at time t Parameters ---------- x0 : np.ndarray Initial image t : int timestep return_noise : bool Whether to also return the epsilon noise added Returns ---------- Noised version of x0 at timestep t \"\"\" if t == 0: return x0 x0 = tf.cast(x0, tf.float32) mean = tf.cast(tf.sqrt(alpha_bar), tf.float32) * x0, var = tf.cast((1-alpha_bar), tf.float32) eps = tf.random.normal( shape=x0.shape ) noised = mean + tf.sqrt(var) * eps if not return_noise: return noised else: return noised, epsThe following image depicts an example of sampling from $q$ at several different timesteps:Diffusion process example.Actually build the netnet = build_unet_model()net.compile(optimizer='adam', loss='mse')Define the dataloader, which will sample a random $x_t$ image and return (X, y) tuples with $(x_{t+1}, \\epsilon)$.BATCH_SIZE = 64shape = (64,64)x_0 = tf.cast(q(lena, 0), tf.float64)def data_generator(): for i in range(1024): # Get a random timestep t = tf.random.uniform(shape=(), minval=1, maxval=len(betas)-1, dtype=tf.int32) # Sample x_{t+1} and also get the noise epsilon that was added to it q_1, noise = q(lena, t, return_noise=True) # Ensure all shapes are correct q_1 = tf.reshape(q_1, shape) noise = tf.reshape(noise, shape) yield q_1, noise# Build dataset from the above generatordataset = tf.data.Dataset.from_generator( data_generator, output_signature = (tf.TensorSpec(shape=shape, dtype=tf.float32), tf.TensorSpec(shape=shape, dtype=tf.float32))).batch(BATCH_SIZE).prefetch(2) Now, we‚Äôre able to train the model.hist = net.fit(dataset, epochs=400)Now we need to implement an inference function which accepts random noise (or any image) and let the process use our model for running the reverse diffusion part.def inference(x, steps=len(betas)): # Save steps for plotting them later iterations = [x] for s in range(steps): t = steps - s beta = betas[t] alpha = alphas[t] alpha_bar = alphas_bars[t] # Predict added noise using our trained model eps = tf.squeeze( net(tf.reshape(iterations[-1], (1,64,64,1))) ) # Get x_{t-1} (algorithm 2) mu = tf.squeeze( (1 / alpha) * ( iterations[-1] - (beta / np.sqrt(1 - alpha_bar)) * eps) ) z = tf.random.normal(shape=mu.shape) if t &gt; 1 else 0 new_img = mu + (z * np.sqrt(beta)) iterations.append(new_img) return iterationsOkay, with all implemented, let‚Äôs start from a very noisy image and see whether it denoises it rightfrom_step = 180plt.imshow(1 - q(lena, from_step).numpy().squeeze(), cmap='Greys', interpolation='nearest')plt.axis('off')Run inferenceresults = inference( q(lena, from_step).numpy(), steps=from_step)Next figure shows an animation (concatenated images in results) of the inference process:Improvements and further researchSince the publication of the paper several steps forward have been made.One drawback of the method explained above is the way inference process works, as sampling one step at a time is clearly a bottleneck. With the intention of mitigating issue this, works like Song et al. 2021, Denoising Diffusion Implicit Models propose tricks aimed at improving inference speed.Another interesting improvement is having more control on the generation process. For example, we could be only interested in generating dog pictures. An quite cool contribution on this is GLIDE, where a transformer is used for encoding a query text (e.g. ‚Äúa dog‚Äù) and then combining that text representation with the internal U-Net activations. This way the image generation process becomes conditioned on what the user specifies via text, gaining more control.Interesting resourcesHere I link several extra resources I found very useful when learning about diffusion models (just in case you want to go down the rabbit hole).[1] What are diffusion models?[2] Ho et al (2020). ‚ÄúDenoising Diffusion Probabilistic Models‚Äù. The original paper.[3] Ramesh et al (2022). DALL-E 2 paper.[4] Imagen, the DALL-E 2 competitor from Google Brain, explainedHope you‚Äôve learned something new today!" }, { "title": "The magic behind (forward mode) Automatic Differentiation", "url": "/posts/forward-mode-autodiferentiation/", "categories": "ML, Deep Learning, Maths", "tags": "maths, programming, python", "date": "2022-03-18 04:33:00 +0100", "snippet": "Demystifying Automatic DifferentiationIf you recall from highschool you‚Äôll probably remember the definition of a derivative:\\[\\begin{equation} f'(x) = \\lim_{\\epsilon \\rightarrow 0} \\frac{f(x+\\epsilon) - f(x)}{\\epsilon} \\label{eq:deriv}\\end{equation}\\]Which is the same as saying: ‚Äúwhat‚Äôs the slope of the line which crosses $f(x)$ and $f(x + \\epsilon)$?‚Äù:This $\\epsilon$ is really a perturbation, a small amount of ‚Äúnoise‚Äù that is added to $x$ in order to calculate the slope.What the $\\lim_{\\epsilon \\rightarrow 0}$ tell us is that the smallest $\\epsilon$ is, the more accurate will be our calculation of $f‚Äô(x)$.Ideally, if we had infinitely precise computers, it wouldn‚Äôt be a bad idea to use this method for computing derivatives. Unfortunately, this is not the case. We start having (big) problems as long as we keep reducing $\\epsilon$.How does a computer read numbers?Why is that? Why we cannot simply reduce $\\epsilon$ as far as we want? TL;DR The number of bits we have for representing numbers is finite, so, inevitably, we incur in rounding errors which create great numerical instability.Computers use the IEEE 754 standard for representing floating point numbers (i.e. numbers in $\\mathbb{R}$). For example, with 32 bits, a number is represented in the following way:This limits out ‚Äúresolution‚Äù to about $10^{-126}$, which is the smallest absolute number (apart from $0$) we can represent.Simplifying with an example. Imagine we only have bits for representing up to two significative figures. If we, for example had $\\epsilon=0.25$ and we wanted to calculate $\\frac{\\epsilon}{2} = 0.125$, which is not representable with two decimals, we thus would have to round, having $\\epsilon=0.13$.As you see, we would have introduced an error of $0.005$ ($= 2\\%$ w.r.t. the true value). It isn‚Äôt difficult to imagine this same error repeating when we have 32 bits. (Obviously, this error will also happen if we use larger numbers like 64 or 128 bit floats, but later).Another problem arises when we compute the derivative using the aforementioned formula $\\eqref{eq:deriv}$. Typically, in comparison, the numerator quantity ($f(x+\\epsilon) - f(x)$) will be much larger than the denominator ($\\epsilon$). In a nutshell, this is also bad regarding roundoff errors.A possible mitigation for these type of errors could be using two numbers at once: one for the non-epsilon related parts of the computation and other for the things which involve epsilon. In other words, separating the perturbation-related computations from the rest.How math solves our problem: Enter the Dual NumbersSo, two numbers at once‚Ä¶ Doesn‚Äôt that sound like imaginary numbers? ($a+ ib; a,b \\in \\mathbb{R}$). Quite a bit, they‚Äôre 2-dimensional numbers which we operate segregated by dimension (i.e. the imaginary part $i$ never interacts with the real part $a$).As with imaginary numbers, when mathematicians face problems, they tend to construct crazy (but ingenuous nonetheless) settings in which their problems can be easily solved. This is the case of a branch of mathematics called Smooth Infinitesimal Analysis, from where the concept of Dual Numbers arise.Dual numbers are an extension to the real numbers and they‚Äôre similar to the imaginary ones in the sense that both have ‚Äú2 dimensions‚Äù which are independent.A dual number takes the form of\\[a + b \\epsilon;\\hspace{1em} a, b \\in \\mathbb{R}\\]with the property that $\\epsilon$ is nilponent, which in layman‚Äôs terms means $\\epsilon^2 = 0$. Although it can seem that Dual and Imaginary numbers are related, this is not the case. Dual numbers are meant to work with calculus involving really, really small numbers. As simple as I can explain it (probably not rigorous): The reason why $\\epsilon^2 = 0$ is that if you take the square of a really really small number, it becomes so small that we can basically consider it 0.Wait, you were talking about derivativesIndeed. Dual numbers, being an extension of reals, allow us to compute both the evaluation of a function $f(x)$ and its derivative $f‚Äô(x)$ in one run!But, how? Let‚Äôs see an example.Consider a polynomial series:\\[P(x) = p_0 + p_1 x + p_2 x^2 + p_3 x^3 + ... + p_n x^n\\]If we plug $x = a + b\\epsilon$, see what happens:\\[P(a+b\\epsilon) = p_0 + p_1 (a+b\\epsilon) + p_2 (a+b\\epsilon)^2 + p_3 (a+b\\epsilon)^3 + ... + p_n (a+b\\epsilon)^n\\]Expanding and reordering the terms‚Ä¶\\[P(a+b\\epsilon) = p_0 + p_1 a + p_2 a^2 + p_n a^n \\fcolorbox{black}{red}{+} p_1 b\\epsilon + 2p_2ab\\epsilon + 3 p_3a^2b\\epsilon + np_n a^{n-1}b\\epsilon\\]Note how the original $P(x)$ evaluation is on the left of the red $\\fcolorbox{black}{red}{+}$ while on the right is $P‚Äô(x)b\\epsilon$. ü§ØNaturally, this effect extends to Taylor series For those of you who don‚Äôt remember what a Taylor series is. It‚Äôs a way of approximating functions around a point $p$ using the derivatives of the same function. They look like: $f(x) = f(x) + \\frac{f‚Äô(a)}{1!}(x - p) + \\frac{f^{\\prime\\prime}(a)}{2!}(x - p)^2 + ‚Ä¶ + \\frac{f^n(a)}{n!}(x - p)^n$. If you want to learn more about them, I‚Äôll refer you to this wonderful 3Blue1Brown video.\\[f(a+b\\epsilon) = f(a) + f^\\prime (a)b\\epsilon + \\frac{f^{\\prime\\prime}(a)}{2!}(b^2 \\epsilon^2) + \\frac{f^{\\prime\\prime\\prime}(a)}{3!}(b^3 \\epsilon^3) + ...\\]The nice thing is that by definition $\\epsilon^2 = 0$, so all the terms on the series $&gt;=2$ become 0. Leaving us simply with:\\[f(a+b\\epsilon) = f(a) + f^\\prime (a)b\\epsilon\\](Note: In our case, the value of $b$ doesn‚Äôt interest us, so from now on, $b=1$)This, in turn, makes us able to operate functions and still have the evaluation and derivative parts separated!If we have two functions $f$ and $g$,\\[\\begin{align*} f: f(a) + \\epsilon f^\\prime(a) \\\\ g: g(a) + \\epsilon g^\\prime(a)\\end{align*}\\]we can operate them as:\\[\\begin{align*}f + g = \\big[ f(a) + g(a) \\big] + \\epsilon \\big[ f'(a) + g'(a) \\big]\\\\f \\cdot g = \\big[ f(a) \\cdot g(a) \\big] + \\epsilon \\big[ f(a) \\cdot g'(a) + g(a) \\cdot f'(a) \\big]\\end{align*}\\](the above comes from operating the Taylor expansions using the algebra of dual numbers).Let‚Äôs see a real example using this mathematical tool.Let $f(x) = 3x^2 + x + 1$. We now that it‚Äôs true derivative is $f‚Äô(x) = 6x + 1$.Applying the dual number method we‚Äôve just talked about, we have:\\[\\begin{align*}f(a + \\epsilon) &amp;=&amp; 3(a + \\epsilon)^2 + (a + \\epsilon) + 1 \\\\ &amp;=&amp; 3(a^2 + \\cancelto{0}{\\epsilon^2} + 2a\\epsilon) + a + \\epsilon + 1 \\\\ &amp;=&amp; 3a^2 + a + 1 + 6a\\epsilon + \\epsilon\\\\ &amp;=&amp; \\underbrace{(3a^2 + a + 1)}_{f(a) \\text{part}} \\fcolorbox{black}{red}{+} \\underbrace{\\epsilon(6a + 1)}_{f'(a) \\text{part}}\\end{align*}\\]Then, we just have to substitute $a$ for the specific value we want and we get both the function evaluation and its derivative at the point $a$!ü§ØMath is cool, but how can I implement this?Ok. Let‚Äôs implement a (very simple) automatic differentiation engine using dual numbers. First, we need to define what is a DualNumber, using a class.class DualNumber: def __init__(self, real_part, dual_part): self.real = real_part self.dual = dual_part def __repr__(self): return f\"{self.real} + {self.dual}œµ\"Having defined the __repr__ method we can see more clearly what is inside of a DualNumber object:DualNumber(2, 3)2 + 3œµThen, in order to do anything useful,this dual number class must support the arithmetic operations we defined above.(In order to keep things simple I will only implement addition. The class would look as follows)class DualNumber: def __init__(self, real_part, dual_part): self.real = real_part self.dual = dual_part def __add__(self, other): if isinstance(other, DualNumber): return DualNumber(self.real + other.real, self.dual + other.dual) elif isinstance(other, Number): return DualNumber(self.real + other, self.dual) else: raise TypeError(\"Cannot add that to a dual number!\") def __radd__(self, other): # This is for when we do things like: 10 + DualNumber() return DualNumber(self.real + other, self.dual) def __repr__(self): return f\"{self.real} + {self.dual}œµ\"Let‚Äôs define a $sin$ function which can accept with DualNumbersimport numpy as npdef sin(x): if isinstance(x, DualNumber): return DualNumber(np.sin(x.real), np.cos(x.real) * x.dual) else: return np.sin(x)With al the above done, we can evaluate $sin(x)$ and $sin‚Äô(x)$ in one single run!func_evals = []deriv_evals = []xticks = np.linspace(-5,5, 100)for i in xticks: x = sin(DualNumber(i, 1)) func_evals.append(x.real) deriv_evals.append(x.dual)# visualize the function and its derivativefig, ax = plt.subplots(figsize=(15,7))ax.plot(xticks, func_evals, label=\"sin(x)\")ax.plot(xticks, deriv_evals, label=\"sin'(x)\")ax.legend()Although it‚Äôs still necessary to add support for the other operations (sub, mult, div, ‚Ä¶), it‚Äôs pretty impressive for this few lines, right?" } ]
